<div align="center">
  <img src="https://github.com/user-attachments/assets/ccb6f5f1-0e07-4eb2-aa7c-5f681c57a59c" alt="Descri√ß√£o da imagem" width="1000"/>
</div

<h1 align="center">Oh no, I¬¥m falling in dropout again üìâ:</h1>

<h3 align="center">Implementa√ß√£o de Dropout em uma MLP feita em Python puro</h3>

<p align="center"><strong>Autoras:</strong> J√∫lia Guedes A. dos Santos e Maria Emily Nayla Gomes da Silva</p>
<p align="center"><strong>Orientador:</strong> Prof. Dr. Daniel R. Cassar</p>

<p align="center">
<img loading="lazy" src="http://img.shields.io/static/v1?label=STATUS&message=EM%20DESENVOLVIMENTO&color=GREEN&style=for-the-badge"/>
</p>


## üìù Descri√ß√£o
<p align="justify">
Neste projeto, nosso objetivo foi implementar o regularizador <em>dropout</em> em uma MLP desenvolvida em Python puro. Al√©m disso, implementamos, em outro notebook, a estrat√©gia de <em>Monte Carlo Dropout</em>, bem como a computa√ß√£o da incerteza nas previs√µes. Na primeira parte do projeto, propusemo-nos a evitar o overfitting da MLP zerando aleatoriamente alguns neur√¥nios com base em uma vari√°vel <strong>P</strong>, que representa a probabilidade (entre 0 e 1) de um neur√¥nio ser desativado. Dessa forma, a cada itera√ß√£o, sorteamos um valor entre 0 e 1 e, caso esse valor seja menor que <strong>P</strong>, o neur√¥nio √© zerado. Caso contr√°rio, ele permanece ativo e segue normalmente o fluxo de computa√ß√£o.
</p>


## üìî Notebooks e arquivos do projeto
* `Imagens`: Pasta contento figuras utilizadas no README e o c√≥digo para gerar a imagem de visualiza√ß√£o do *dataset*.
  - `logos_Ilum-CNPEM.jpg`: logotipos da institu√ß√£o na qual tal projeto foi realizado e seus v√≠nculos.
* `README.md`: descri√ß√£o geral do projeto.


## üóÇÔ∏è Dataset - Breast Cancer Dataset (Kaggle)
<p align="justify">
  O dataset utilizado para testar o desempenho do regularizador <em>dropout</em> foi o <em>Breast Cancer Dataset</em>, encontrado no site <em>Kaggle</em>. 
  Nesse dataset, o target escolhido √© uma vari√°vel nominal que indica o tipo de tumor do paciente, ou seja, se o tumor √© benigno (B) ou maligno (M).
</p>

<div align="center">

| Colunas              | Descri√ß√£o                                                                                      |
|----------------------|-----------------------------------------------------------------------------------------------|
| `id`                 | Identifica√ß√£o do paciente                                                                      |
| `diagnosis`          | Target; classifica√ß√£o do tumor em benigno e maligno                                             |
| `radius_mean`        | Raio dos l√≥bulos das mamas                                                                      |
| `texture_mean`       | M√©dia da textura da superf√≠cie                                                                  |
| `perimeter_mean`     | Per√≠metro externo do l√≥bulo                                                                    |
| `area_mean`          | √Årea m√©dia do l√≥bulo                                                                            |
| `smoothness_mean`    | M√©dia da suavidade dos contornos                                                                |
| `compactness_mean`   | M√©dia da compacidade                                                                            |
| `concavity_mean`     | M√©dia da concavidade (profundidade das regi√µes c√¥ncavas dos contornos)                         |
| `concavity_points`   | M√©dia da quantidade de pontos de concavidade nos contornos                                      |

</div>



## üèãÔ∏è‚Äç‚ôÄÔ∏è Implementando o dropout
<p align="justify">
A seguir, √© poss√≠vel observar as classes modificadas na MLP utilizada como refer√™ncia. Com a MLP implementada em Python puro, sab√≠amos que era necess√°rio criar um mecanismo para zerar gradientes aleatoriamente, levando em conta uma probabilidade previamente definida ‚Äî ou seja, um hiperpar√¢metro utilizado para definir como a rede neural ser√° regularizada. Dessa modo, estar√≠amos implementando o regularizador <em>dropout</em> em nossa rede, atuando para evitar o overfitting no modelo que est√°vamos desenvolvendo.
</p>

<p> </p>

### ü™¥ Modifica√ß√µes no c√≥digo 
#### üå± No momento de intanciar a MLP
<p align="justify">
  Nesta primeira c√©lula, definimos o hiperpar√¢metro <em>P</em> e instanciamos a vari√°vel <strong>minha_mlp</strong> com os seguintes argumentos: <em>NUM_DADOS_DE_ENTRADA</em> ‚Äî a dimens√£o dos dados do dataset de treino; <em>arquitetura_da_rede</em> ‚Äî uma lista que informa a quantidade de neur√¥nios em cada camada oculta; e o pr√≥prio <em>P</em>.
</p>

```python
P = 0.2
minha_mlp = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede, P)
```

#### üå± Na classe MLP
<p align="justify">
Na c√©lula seguinte, que √© a parte de interesse para o coment√°rio sobre as modifica√ß√µes na classe MLP, criamos a lista <em>camadas</em> como anteriormente. No entanto, para as camadas de entrada e de sa√≠da, instanciamos a vari√°vel <em>camada</em> com um par√¢metro adicional, que serve para indicar que essas camadas n√£o devem passar pelo <em>dropout</em>.
</p>

```python
class MLP:
    def __init__(self, num_dados_entrada, num_neuronios_por_camada, dropout_p = 0):
        percurso = [num_dados_entrada] + num_neuronios_por_camada
        
        camadas = []
        tamanho = len(num_neuronios_por_camada)
        for i in range(tamanho):
            if i == 0 or i == tamanho-1:
                camada = Camada(num_neuronios_por_camada[i], percurso[i], self.dropout_p)
            else:
                camada = Camada(num_neuronios_por_camada[i], percurso[i], self.dropout_p, 1) # Quando √© uma camada de entrada ou de sa√≠da
            camadas.append(camada)
            
        self.camadas = camadas
```
#### üå± Na classe Camada
<p align="justify">
Na classe <em>MLP</em>, a vari√°vel <em>camada_visivel</em> indica se est√° sendo instanciada uma camada oculta ou uma camada vis√≠vel (entrada ou sa√≠da).  Caso seja uma camada de entrada ou de sa√≠da, indicada por <em>camada_visivel = 1</em>, nenhum neur√¥nio ser√° zerado, ou seja, essa camada n√£o passar√° pelo <em>dropout</em>.</p>


```python
class Camada:
    def __init__(self, num_neuronios, num_dados_entrada, dropout_p = 0, camada_visivel = 0):
        neuronios = []
        
        for _ in range(num_neuronios):
            neuronio = Neuronio(num_dados_entrada)
            neuronios.append(neuronio)
            
        self.neuronios = neuronios
        self.dropout_p = dropout_p
        self.camada_visivel = camada_visivel
        
    def __call__(self, x):
        dados_de_saida = []
        
        for neuronio in self.neuronios:
            # Caso n√£o seja uma camada vis√≠vel e o random.random() seja menor que a probabilidade P
            if self.camada_visivel == 0 and random.random() < self.dropout_p: 
                informacao = Valor(0)
            else:
                informacao = neuronio(x)
            dados_de_saida.append(informacao)
            
        if len(dados_de_saida) == 1:
            return dados_de_saida[0]
        else:        
            return dados_de_saida 
```


## üî¢ Resultados obtidos
<p align="justify"> A curva de aprendizado foi um dos m√©todos que utilizamos tanto para avaliar o quanto o modelo estava aprendendo durante as √©pocas quanto para verificar se o regularizador avaliado ‚Äî o dropout ‚Äî estava sendo efetivo na generaliza√ß√£o da rede. Ao conseguir generalizar a rede, haveria uma piora na perda do treino e uma diminui√ß√£o da perda dos dados de valida√ß√£o. Como esperado, houve queda na loss, e a dos dados de valida√ß√£o foi menor do que a dos dados de treino, o que nos levou a inferir que o regularizador foi implementado com sucesso. Contudo, como a acur√°cia encontrada foi de $64,91\%$, a mesma do modelo Dummy, conclu√≠mos que ainda n√£o encontramos a melhor estrat√©gia para lidar com os dados do notebook.
</p>

## üòÅ Conclus√£o
<p align="justify">
Ao final, como esperado a partir da implementa√ß√£o do regularizador Dropout, os dados de valida√ß√£o apresentaram melhor desempenho em compara√ß√£o aos dados de treino. No entanto, a acur√°cia encontrada para a rede treinada foi igual a de um modelo Dummy, o que pode significar que a estrutura n√£o foi complexa o suficiente para encontrar par√¢metros robustos que pudessem capturar a complexidade dos dados. Portanto, tem-se que apesar da implementa√ß√£o bem-sucedida do dropout, foi poss√≠vel concluir que essa estrat√©gia n√£o foi eficaz para o dados utilizados no notebook.
</p>
<div align="center">
  <img src="Imagens/curva_aprendizado.png" alt="Descri√ß√£o da imagem" width="1000"/>
</div>

## üñáÔ∏è Informa√ß√µes t√©cnicas
* Linguagem de programa√ß√£o: `Python 3.9`
* Software:  `Jupyter Notebook`
* Bibliotecas e M√≥dulos: `zipfile`,`Random`, `Matplotlib`, `os`, `Pandas`, `Math`, `scikit-learn`
<br>

## üë©‚Äçü¶≥ Refer√™ncias
[1] DropOut Layer. NumPyNet. Dispon√≠vel em: <https://nico-curti.github.io/NumPyNet/NumPyNet/layers/dropout_layer.html>. Acesso em: 19 abr. 2025.
 
[2] DSA, Equipe. Cap√≠tulo 23 - Como Funciona o Dropout? Deep Learning Book. Dispon√≠vel em: <https://www.deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/>. Acesso em: 19 abr. 2025.
 
[3] Refer√™ncia Principal (constru√ß√£o da rede neural em Python puro): Andrej Karpathy. The spelled-out intro to neural networks and backpropagation: building micrograd (2022). https://www.youtube.com/watch?v=VMj-3S1tku0


## üß† Contribui√ß√µes dos Colaboradores
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/172424779?v=4" width=115><br><sub>Julia Guedes A. Santos</sub>](https://github.com/JuliaGuedesASantos)<br> [<sub>Ilum - CNPEM</sub>](https://ilum.cnpem.br/)<br> [<sub>Curr√≠culo Lattes</sub>](https://lattes.cnpq.br/9504021537643847)<br> [<sub>Linkedin</sub>](https://www.linkedin.com/in/j%C3%BAlia-guedes-546542283/) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/172424897?v=4" width=115><br><sub> Maria Emily Nayla</sub>](https://github.com/MEmilyGomes)<br> [<sub>Ilum - CNPEM</sub>](https://ilum.cnpem.br/)<br> [<sub>Curr√≠culo Lattes</sub>](http://lattes.cnpq.br/9482558334105708)<br> | [<img loading="lazy" src="https://github.com/user-attachments/assets/463d4753-7fa4-4a42-aa54-409e4150bb51" width=115><br> <sub> Prof. Dr. Daniel R. Cassar </sub>](https://github.com/drcassar)<br> [<sub>Ilum - CNPEM</sub>](https://ilum.cnpem.br/)<br> [<sub>Curr√≠culo Lattes</sub>](http://lattes.cnpq.br/1717397276752482) | 
| :---: | :---: | :---: | 

#### Para o Projeto:
* Emily Gomes: Implementa√ß√£o do dropout em uma MLP em Python puro.
* J√∫lia Guedes: Implementa√ß√£o do dropout em uma MLP em Python puro e coment√°rio do c√≥digo.

#### Para o Reposit√≥rio GitHub:
* Emily Gomes: README
* J√∫lia Guede: 

**Orienta√ß√£o:** Prof. Dr. Daniel R. Cassar.


